Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech synthesizer and can be implemented in a software or hardware product. Text-to-speech (TTS) systems convert normal language text to speech, while other systems render symbolic language representations to speech, such as voice transcription. The reverse process is speech recognition.
Synthetic speech can be created by concatenating fragments of recorded speech that are stored in the database. The system that stores the phone or diphone provides the maximum output range, but may lack clarity. For a specific range of usage, the storage of the entire word or sentence allows for high quality output. Alternatively, the synthesizer can incorporate models of the vocal tract and other human speech characteristics to create a fully "synthesized" speech output.
The quality of the speech synthesizer is judged by its similarity to the human voice and its ability to clearly understand. Easy-to-understand text-to-speech programs allow people with visual or reading disabilities to hear words written on their home computers. Many computer operating systems have included speech synthesizers since the early 1990s.
The text-to-speech system (or "engine") consists of 2 parts: front-end and back-end. The front end has 2 main tasks. First, convert the raw text, including symbols such as numbers and abbreviations, into the equivalent of the exported word. This process is often referred to as text normalization, preprocessing, or tokenization. The front end then assigns a speech transcription to each word, splits and marks the text into prosodic units like phrases, clauses, and sentences. The process of assigning speech transcription to words is called text-to-phoneme conversion or grapheme-to-phoneme conversion. Speech transcription and prosodic information constitute a symbolic linguistic representation that is output by the front end. The back-end (often referred to as a synthesizer) converts symbolic language representations into sounds. In certain systems, this part includes the calculation of the target prosody (pitch contour, phoneme duration), which is imposed on the output speech.
History
Long before the invention of electronic signal processing, some people tried to build machines to emulate human speech. Some early legends of the existence of the "brazen head" involved Pope Sylvester 2 (circa 1003 A.D.), Albertus Magnus (1198-1280) and Roger Bacon (1214-1294).
In 1779, German and Danish scientist Christian Gottlieb Kratzenstein won the first prize in a competition presented by the Russian Imperial Academy of Sciences and Arts for a model that constructed the human vocal tract capable of producing five long vowels. It followed the bellows "acoustic mechanical voice machine" described in a 1791 paper by Wolfgang von Kemperen of Pressburg, Hungary. The machine added models of tongue and lips, allowing to produce not only vowels, but also consonants. In 1837, Charles Wheatstone created a "talking machine" based on von Kemperen's design, and in 1846 Joseph Faber exhibited "Euphonia". In 1923, Padgett revived Wheatstone's design.
In the 1930s the Bell Institute developed a vocoder, which automatically analyzes speech to its basic tone and resonance. From vocoder work, Homer Dudley developed a keyboard-operated voice synthesizer called Voder (Voice Demonstrator), which was exhibited at the 1939 World's Fair in New York.
Franklin S. Haskins LaboratoriesDr. Cooper and his colleagues built the pattern reproduction in the late 1940s and completed it in 1950. There were several different versions of this hardware device, but now only 1 survived. The machine converts a picture of the acoustic pattern of speech into sound in the form of a spectrogram. Alvin Liberman and colleagues used the device to discover acoustic cues for the perception of speech segments (consonants and vowels).
Electronic devices
The first computer-based speech synthesis system was born in the late 1950s.Noriko Umeda et al. In 1968, the Japanese Institute of Electrical Technology developed the first common English text-to-speech system. In 1961, physicist John Larry Kelly, Jr and his colleague Louis Gerstman synthesized speech using the IBM704 computer. Kelly's voice recorder synthesizer (vocoder) recreated the song "Daisy Bell" with the accompaniment of Max Matthews. coincidentally, arthur C. Clark was visiting his friend and colleague John Pearce at the Bell Institute's Murray Hill facility. Clark was so impressed by the demonstration that he used it in the climax scene of his screenplay for his novel 2001: A Space Odyssey, HAL9000 computer sings the same song that astronaut Dave Bowman puts it to sleep. Despite the success of purely electronic speech synthesis, research on mechanical speech synthesis continues.
Linear predictive coding (lpc), a form of speech coding, was developed in 1966 by Fumitada Itakura of Nagoya University and Shuso Saito of Nippon Telegraph and Telephone (NTT). Further development of LPC technology was achieved in the 1970s by Bishnu S. S. of the Bell Institute.Atal and Manfred R.It was done by Schroeder.The LPC later became the basis for early speech synthesizer chips, such as the Texas Instruments LPC Speech chip used in Speak & Spell toys since 1978.
In 1975, Fumitada Itakura developed a line spectrum pair (lsp) method for high compression speech coding while at NTT. From 1975 to 1981, Itakura studied the problems of speech analysis and synthesis based on the LSP method. In 1980, his team developed an LSP-based speech synthesis chip. LSP is an important technology for speech synthesis and coding, and was adopted as an essential component of almost all international speech coding standards in the 1990s, contributing to the enhancement of digital voice communication over mobile channels and the Internet.
In 1975, MUSA was released and was one of the first speech synthesis systems 1. It consisted of stand-alone computer hardware and special software that allowed it to read Italian. Released in 1978, the 2nd version was also able to sing Italian in the "A cappella" style.
The dominant systems of the 1980s and 1990s were mainly the DECtalk and Bell Labs systems based on the work of Dennis Klatt at MIT, the latter being one of the first multilingual language independent systems to make extensive use of natural language processing methods.
Handheld electronics, characterized by speech synthesis, began to appear in the 1970s. 1976, voice + portable calculator for the visually impaired. Other devices had primarily educational purposes, such as the Speak & Spelling toy manufactured by Texas Instruments in 1978. Fidelity released a speaking version of the electronic chess computer in 1979. The first video game to feature speech synthesis was the shoot' em up arcade game Stratovox (known in Japan as Speak & Rescue), launched by sun Electronics in 1980. The first personal computer game using speech synthesis was a shoplifting girl released in 1980 for PET2001, and the game's developer, Hiroshi Suzuki, developed a "zero-cross" programming technique to generate synthetic speech waveforms. Another early example, the arcade version of berzerk, also dates back to 1980. The Milton Bradley Company produced the first multiplayer electronic game, Milton, using speech synthesis in the same year.
Early electronic voice synthesizers often sounded like robots and were almost incomprehensible. The quality of synthesized speech is steadily improving, but as of 2016, the output from modern speech synthesis systems can be clearly distinguished from real human speech.
The synthesized voice was typically a male voice until 1990, when Ann Syrdal of the AT&T Bell Labs created the female voice.
Kurzweil predicted in 2005 that more people would benefit from the use of text-to-speech programs because the cost-performance ratio would make speech synthesizers cheaper and more accessible.
Synthesizer Technologies
The most important quality of speech synthesis systems is naturalness and clarity. Naturalness represents how closely the output sounds like human speech, while clarity is the ease with which the output is understood. The ideal voice synthesizer is natural and easy to understand. Speech synthesis systems usually try to maximize both characteristics.
The two main technologies for generating synthetic speech waveforms are connected synthesis and formant synthesis. Each technique has its advantages and disadvantages, and the intended application of the synthetic system usually determines which approach will be used.
Linked synthesis
Concatenation synthesis is based on the concatenation (stringing) of segments of recorded speech. In general, linked synthesis produces the most natural sounding synthetic voice. However, the difference between the natural fluctuations of speech and the nature of automated techniques for segmenting waveforms can lead to audible glitches in the output. There are 3 main subtypes of linked synthesis.
Unit selection synthesis
Unit selection synthesis uses a large database of recorded audio. During the creation of the database, each recorded utterance is divided into individual phones, difons, halffons, syllables, morphemes, words, phrases, and some or all of the sentences. Typically, the division into segments is done using visual representations such as waveforms and spectrograms, using a specially modified speech recognition device that is later set to the "forced alignment" mode with manual correction. The index of units in the voice database is created based on segmentation and acoustic parameters such as fundamental frequency (pitch), duration, position in syllables, and adjacent phones. At runtime, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is usually accomplished using specially weighted decision trees.
Unit selection provides maximum naturalness because it applies only a small amount of digital signal processing (DSP) to the recorded voice. DSP often uses a small amount of signal processing at the connection point to smooth the waveform, but the sound of the recorded voice is not very natural and the output from the best unit selection system is often indistinguishable from the real human voice, especially in situations where the TTS system is tuned. But the greatest naturalness is that in some systems, ranging from gigabytes of recorded data, usually representing dozens of hours of speech, very large unit selection speech day Also unit selection algorithms can select segments from where the ideal synthesis will be less than (e.g., minor words become unclear), even if a better selection exists in the database. It is known that. In recent years, various automated methods for detecting unnatural segments have been proposed in the unit selection speech synthesis system.
Diphone synthesis
Diphone synthesis uses a minimal audio database that contains all diphone (sound-to-sound transitions) that occur in the language. For example, Spanish has about 800 difons, and German has about 2500 difons. In Diphone synthesis, only 1 example of each diphone is included in the voice database. At runtime, the target prosody of the sentence is superimposed on these minimum units by digital signal processing techniques such as linear predictive coding, PSOLA or MBROLA. Or more recent techniques such as pitch correction in the source domain using discrete cosine transformations. Diffon synthesis suffers from the sound glitches of concatenated synthesis and the robotic sound nature of formant synthesis, and there are few advantages of any approach other than the small size. As a result, although its use in commercial applications is decreasing, it continues to be used in research because of the large number of freely available software implementations. An early example of diphone synthesis is Michael J.It is the teaching robot Leachim, invented by Freeman. Leachim included information about the curriculum of the class and specific biographical information about the students who were programmed to teach. It was tested in a 4th grade classroom in the Bronx, New York.
Domain-specific synthesis
Domain-specific compositing concatenates pre-recorded words and phrases to create a complete utterance. This is because the variety of text the system outputs is limited to certain domains, such as transit schedule announcements and weather forecasts applications This technology is very simple to implement and has long been used commercially in devices like call clocks and calculators. The level of naturalness of these systems can be very high, due to the limited variety of sentence types, which can affect the prosody and intonation of the original recording.
These systems are limited by words and phrases in the database, so they can only synthesize combinations of pre-programmed words and phrases, rather than generic. For example, in the non-rhotic dialects of English, the "r" of a word like "clear"/ˈ kl t/ is usually pronounced only if the next word has a vowel as the first letter. Similarly in French, when followed by a word that begins with a vowel, many last consonants are no longer silenced and have an effect called a liaison. This alternation is not reproducible in a simple word-concatenation system, which requires more complexity to be context-sensitive.
Formant synthesis
Formant synthesis does not use human speech samples at runtime. Instead, the synthesized audio output is created using additive synthesis and acoustic models (physical modeling synthesis). Parameters such as fundamental frequency, voicing, and noise level change over time to create waveforms of artificial speech. This method is sometimes called rule-based synthesis, but many concatenated systems also have rules-based components. Many systems based on formant synthesis technology produce artificial robotic sounds that are not mistaken for human speech. However, maximum naturalness is not necessarily the goal of speech synthesis systems, and formant synthesis systems have advantages over concatenated systems. Formant synthesized speech is reliably understandable, even at very high speeds, and avoids acoustic glitches that plague the coupling system. High-speed synthetic speech is used by visually impaired people to quickly navigate the computer using a screen reader. Formant synthesizers are usually smaller programs than concatenated systems because they do not have a database of voice samples. They can therefore be used in embedded systems where memory and microprocessor power are particularly limited. The formant-based system has full control over all aspects of the output voice, so it can output a wide variety of prosodic and intonation, not just questions and statements, but also conveying different emotions and tone of voice.
Examples of non-real-time, high-precision intonation control in formant synthesis include work done at Texas Instruments' toy Speak&Spell in the late 1970s, and at Sega Arcade Machines and many Atari companies in the early 1980s. Arcade game using tms5220LPC chip. Creating the right intonation for these projects is painstaking and the result is still real-time text-to-speech.
Articulation synthesis
Articulation synthesis refers to a computational technique for synthesizing speech based on a model of the human vocal tract and the articulation process that occurs there. The first articulatory synthesizer regularly used for laboratory experiments, known as Ha ASY in the mid-1970s by Philip Rubin, Tom Bear, and Paul Marmelstein This synthesizer was developed by Paul Mermelstein, Cecil Coker et al at Bell Labs in the 1960s and 1970s. It was based on the vocal tract model.
Until recently, articulatory synthesis models were not incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system, developed and marketed by Trillium Sound Research, a spin-off company from the University of Calgary, where much of the original research was done. Following the demise of various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), Trillium software was released under the GNU General Public License and continued to work as gnuspeech. First marketed in 1994, the system, using waveguides or transmission line analogues of human oral and nasal tubes controlled by Carré's "distinct region model", provides complete articulation-based text-to-speech conversion.
This time, Jorge C.A recent synthesizer developed by Lucero and colleagues incorporates models of vocal cord biomechanics, glottal aerodynamics, and sound propagation in the bronchi, trachea, nasal cavity, and oral cavity, making up a complete system of physics-based speech simulations.
HMM-based Synthesis
HMM-based synthesis is a synthesis method based on the hidden Markov model, also called statistical parametric synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (speech source), and duration (prosody) of speech were modeled simultaneously by Hmms. The speech waveform is generated from the Hmm itself based on the maximum likelihood criterion.
Cinewave synthesis
Sinusoidal synthesis is a technique for synthesizing speech by replacing the formant (the main band of energy) with a whistle of pure sound.
Deep Learning-based Synthesis
Deep Learning Speech synthesis uses a deep neural network (DNN) to generate artificial speech from text (text to speech) or spectrum (vocoder). Deep neural networks are trained using a large amount of recorded speech, and in the case of text-to-speech systems, the associated labels and/or input text-to-speech (text-to-speech) can be used to identify the relevant labels and/or input text-to-speech (text-to-speech).
DNN-based speech synthesizers are getting closer to the naturalness of the human voice. Examples of the disadvantages of this method are low robustness when the data is not enough, lack of controllability, and low performance in automatic regression models.
Tonal languages such as Chinese and Taiwanese require different levels of tonal sandy, and the output of the voice synthesizer can lead to tonal sandy mistakes.
Audio Deep Fake
Audio Deep fake (also known as voice cloning) makes compelling speech sentences that sound like saying things that a particular person did not say.This technique was originally developed for various uses to improve human life. For example, it can be used to create audiobooks or to help people who have lost their voice (due to throat diseases or other medical problems) regain their voice. Commercially, it opened the door to several opportunities. This technology can also create more personalized digital assistants and natural sounding text-to-speech as well as voice translation services.
Audio deep fakes, more recently called audio manipulation, have become widely accessible using simple mobile devices and personal computers. These tools are also used to spread false information using voice. This has led to cybersecurity concerns among people around the world about the side effects of using audio deep fakes, including its potential role in spreading misinformation and disinformation on audio-based social media platforms. People use them as logical access voice spoofing techniques that can be used to manipulate public opinion for propaganda, defamation, or terrorism. Audio Deep Fake attackers are targeting individuals and organizations, including politicians and governments. In early 2020, some fraudsters used artificial intelligence-based software to impersonate the CEO's voice and authorized the transfer of aboutト3,500 million by phone. According to the 2023 McAfee global survey, 10 out of 1 people reported being targeted by AI voice cloning scams, and 77% of them reported losing money in the scam. Audio deep fakes can also pose a danger to the voice ID system currently deployed to financial consumers.
Challenges
Text Normalization Challenges
The process of normalizing the text is rarely simplified. The text is full of synonyms, numbers, and abbreviations that require extensions to all phonetic representations. English has many spellings and different pronunciation based on context. For example, "My latest project is to learn how to project my voice better" contains 2 pronunciations of "Project".
Most text-to-speech (TTS) systems produce semantic representations of the input text because the process for that is unreliable, poorly understood, and computationally ineffective, resulting in various heuristic techniques being used to infer appropriate ways to obscure the homograph, such as examining adjacent words or using statistics on frequency of occurrence. It is used.
Recently, the TTS system has begun to use Hmm (discussed above) to generate "parts of speech" to help obscure homographs. This technique has been very successful in many cases, such as whether "read" should be pronounced "red", which means the past tense, or "reed", which means the present tense. The typical error rate when using Hmm in this way is usually not more than five percent. These techniques work well in most European languages, but access to the training corpus you need is often difficult in these languages.
Deciding how to convert numbers is another problem that the TTS system has to deal with.1 Converting numbers into words (at least in English) is a simple programming challenge so that "1325" becomes "one thousand three hundred twenty-five". But the numbers occur in many different contexts, and "1325" can also be read as "one three two five", "thirteen twenty-five" or "thirteen hundred and twenty five." The TTS system can guess how to extend numbers based on surrounding words, numbers, and punctuation, and may provide a way to specify context in ambiguous cases. Roman numerals can also be read differently depending on the context. For example, "Henry 8" is read as "Henry 8" and "Chapter 8" is read as "Chapter 8".
Similarly, abbreviations can be ambiguous. For example, the abbreviation "in" for "inch" should be distinguished from the word "in", and the address "12St John St."Use the same abbreviation for both "Saint" and "Street". The TTS system with an intelligent front end allows you to make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, "Ulysses S.A."It results in meaningless (and sometimes comical) output such that "Grant" is rendered as "Ulysses South Grant".
Text-to-Phoneme Challenges
Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, and often the simplest approach to text-to-phoneme or scribbled text-to-phoneme conversion is a dictionary-based approach in which a large dictionary containing all the words of the language and its correct pronunciation is stored by the program. It's a good idea. Determining the correct pronunciation of each word is a matter of searching for each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. Another approach is rule-based, which applies pronunciation rules to words to determine pronunciation based on spelling.1 This is similar to the "echoing", or synthetic phonics approach to learning reading.
Each approach has its advantages and disadvantages. The dictionary-based approach is quick and accurate, but it completely fails if you are given a word that is not in the dictionary. As the size of the dictionary increases, so does the memory space requirements of the synthetic system. On the other hand, the rule-based approach works for any input, but the complexity of the rules increases significantly as the system considers irregular spelling and pronunciation. (The word "of" is very common in English, but consider that the letter "f" is the only word to be pronounced.)As a result, almost all speech synthesis systems use a combination of these approaches.
Languages with phoneme orthography have a very regular way of writing, and it is very successful to predict the pronunciation of words based on their spelling. The speech synthesis system of such languages often extensively uses rule-based methods and relies on dictionaries of a small number of words, such as foreign names and foreign words. On the other hand, speech-to-speech systems in languages with very irregular spelling systems, such as English, are more likely to rely on dictionaries and use rule-based methods only for rare words or words that are not in the dictionary.
Evaluation issues
Consistent evaluation of speech synthesis systems can be difficult due to the lack of universally agreed objective evaluation criteria. Different organizations often use different voice data. The quality of the speech synthesis system also depends on the quality of the production technology (which can include analog or digital recording) and the equipment used to reproduce the speech, therefore, the evaluation of the speech synthesis system has often been compromised by the difference between the production technology and the playback equipment.
However, since 2005, some researchers have begun to evaluate speech synthesis systems using a common voice data set.
Prosodix and Emotional Content
According to a study in the journal Speech Communication by Amy Drahota and colleagues at the University of Portsmouth, UK, listeners of voice recordings can judge at a better level than by chance whether the speaker is laughing or not. It was suggested that the identification of features of the voice that indicate emotional content could be used to make the synthesized speech sound more natural. 1. One of the related problems is the correction of the pitch outline of the sentence, depending on whether it is a positive, questionable, or exclamatory sentence. One of the techniques for pitch correction uses discrete cosine transformations in the source region (linear prediction residuals). Such pitch-synchronous pitch correction techniques require a priori pitch marking of synthetic speech databases using techniques such as epoch extraction using dynamic plosionindex applied to integrated linear prediction residuals of voiced speech regions.
Dedicated hardware
-Icofon
* General Instrument SP0256-AL2
*National Semiconductor DT1050 Digital Car (Moser Forest Moser)
* Texas Instruments LPC Speech Chip
Hardware and Software systems
A popular system that provides speech synthesis as a built-in feature.
Texas Instruments
In the early 1980s, TI was known as a pioneer in speech synthesis.The ti-99/4 and 4A included a very popular plug・in speech synthesizer module.The voice synthesizer was offered for free upon purchase of a large number of cartridges and was used in many TI-written video games (the games offered by voice during this promotion included Alpiner and Parsec). The synthesizer uses a variant of linear predictive coding and incorporates a small vocabulary. The original purpose was to release a small cartridge directly connected to the synthesizer unit. However, the success of the software text-to-speech in the Terminal Emulator II cartridge canceled its plans.
Mattel
The Mattel Intellivision game console provided the Intellivoice Voice integration module in 1982. The removable cartridge included the Sp0256 Narrator voice synthesizer chip. The narrator has a 2KB read-only memory (ROM) and using this, general word data that can be paired to create phrases in Intellivision games, additional words or phrases needed can be stored inside the cartridge itself as the orator chip can also accept speech data from external memory. The data consists of a simple digitized sample rather than a string of analog filter coefficients to change the behavior of the chip's synthetic vocal tract model.
sam.
Also released in 1982, Software Automatic Mouth was the first commercial all-software speech synthesis program. It was later used as the basis for Macintalk. The program was available on non-Macintosh Apple computers (including Apple II, and Lisa), various Atari models, and Commodore64. The Apple version preferred additional hardware, including a Dac, but if the card was not present, it instead used the computer's 1-bit audio output (adding a lot of distortion) Atari utilized an embedded POKEY audio chip. Audio playback in Atari typically disables interrupt requests and shuts down the ANTIC chip during vocal output. The audible output is very distorted audio when the screen is on. The Commodore 64 used 64 embedded SID audio chips.
Atari
Perhaps the first voice system integrated into the operating system was Atari, Inc.Designed by 1400XL/1450XL was a personal computer. In 1983, the Votrax SC01 chip was used. The 1400XL/1450XL computer used a finite state machine to enable the synthesis of the world's English spelling text to speech. Unfortunately, the 1400XL/1450XL personal computer was not shipped in quantity.
The Atari ST computer was sold together with "stspeech"."tos" on the floppy disk.
Apple
The first voice system integrated into the operating system, which was shipped in large quantities, was MacinTalk on Apple Computer. The software was developed by third-party developers Joseph Katz and Mark Barton (later SoftVoice, Inc.It was licensed from ( <url>).It was introduced during the introduction of Macintosh computers in 1984. This 1 month demo required 512 kilobytes of RAM memory. As a result, it couldn't run on the 128 kilobytes of RAM that the first Mac actually shipped with. So the demo was achieved with a prototype 512k Mac, but attendees were not told about this, and the synthetic demo created quite a bit of excitement for the Macintosh. In the early 1990s, Apple expanded its ability to provide text-to-speech support for the entire system. With the introduction of faster PowerPC-based computers, higher quality voice sampling was included. Apple has also introduced speech recognition to its system, providing a fluid command set. More recently, Apple has added a sample-based voice. Starting as a curiosity, the Apple Macintosh voice system has evolved into a fully supported program for people with vision problems, PlainTalk. VoiceOver was first introduced in 2005 with Mac OS X Tiger (10.4). In the first releases of 10.4 (Tiger) and 10.5 (Leopard), there was only one standard voice that shipped with Mac OS X. 1Starting with 10.6 (Snow Leopard), users can choose from a wide range list of several audio VoiceOver voices featuring realistic sonorous breath taking as well as improved clarity between sentences at high read rates over plaintalk. Mac OS X also includes a command line-based application that converts text to audible speech. A standard addition to AppleScript includes a script that uses one of the installed voices and a verb to say that allows you to control pitch, speaking rate and modulation of spoken words.
Amazon
It will be used by Alexa and used as software as a service on AWS (starting in 2017).
Amigaus
The second operating system with advanced speech synthesis capabilities was AmigaOS, which was introduced in 1985. Speech synthesis is available from Commodore International, SoftVoice, Inc.It was licensed from <url>.The who has also developed a text-to-speech system for the original MacinTalk. This is made possible through the Amiga audio chipset, with both male and female voices and "stress" indicator markers, the voice emulation synthesis system for American English has a translation library that converts unlimited English text into a standard set of voice codes, and a narrator that implements a formant model of voice generation. It was divided into two types: the first and the second. AmigaOS also featured a high-level "speaking handler" that allows command-line users to redirect text output to voice. Speech synthesis was sometimes used by third-party programs, especially word processing and educational software. Synthesis software has changed little since the initial AmigaOS release, and Commodore has finally removed support for speech synthesis from amigaos 2.1 and later.
Despite the limitations of phonemes in American English, an unofficial version with multilingual speech synthesis was developed. It made use of an extended version of the translation library that can translate several languages, given a set of rules for each language.
Microsoft Windows
Modern Windows desktop systems can use sapi4 and sapi5 components to support speech synthesis and speech recognition. SAPI4.0 was available as an optional add-on for windows95 and Windows98. Windows2000 has added a narrator, a text-to-speech utility for people with visual impairment. Third-party programs such as JAWS for Windows, Window-Eyes, non-visual desktop Access, Supernova, and System Access can perform various text-to-speech tasks such as specified websites, email accounts, text documents, Windows clipboard, and user keyboard input. You can also use the following methods: Not all programs can use speech synthesis directly. Some programs allow you to read text aloud using plug-ins, extensions, or add-ons. Third-party programs that can read text from the system clipboard are available.
Microsoft Speech Server is a server-based package for speech synthesis and recognition. It is designed for network use with web applications and call centers.
Votrax
From 1971 to 1996, Votrax produced several commercial voice synthesizer components. The Votrax synthesizer was included in the first generation Kurzweil reader for the visually impaired.
Text-to-speech system
Text-to-speech (TTS) refers to the ability of a computer to read text aloud. The TTS engine converts written text into phoneme representations and converts phoneme representations into waveforms that can be output as sound. TTS engine with different languages, dialects and special vocabulary is available through third-party publishers.
Android
Version 1.6 of Android added support for text-to-speech (TTS).
The Internet
Currently, specialized software that is an application, plug-in or gadget that can read messages directly from an email client or web page from a web browser or Google toolbar can tell the RSS feed. On the other hand, online RSS narrators simplify information delivery by allowing users to listen to their favorite news sources and convert them into podcasts. On the other hand, online RSS readers are available on almost any personal computer connected to the Internet. Users can download the generated audio files to portable devices. Listen to them while walking, jogging or commuting to work with the help of a podcast receiver.
A growing area of Internet-based TTS is web-based assistive technologies such as "Browsealoud" from a UK company or Readspeaker. It can provide TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) who has access to a web browser. The nonprofit project Pediaphon was created in 2006 to provide a similar web-based TTS interface to Wikipedia.
Other work has been done by the BBC and Google Inc.With the involvement of the W3C Audio Incubator Group, we are taking place in the context of the W3C through the W3C Audio Incubator Group.
Open Source
Some open source software systems are available as follows:
* eSpeak to support a wide range of languages.
* Festival speech synthesis system using Diphone-based synthesis as well as more modern and better sounding technology.
* gnuspeech using articulation synthesis from the Free Software Foundation.
Other
* Following the commercial failure of hardware-based Intellivoice, game developers used software synthesis sparingly in later games. Previous systems from Atari, such as the Atari5200 (baseball) and Atari2600 (Quadrun and Open Sesame), also had games that utilized software synthesis.
* Some e-book readers, such as Amazon Kindle, Samsung E6, PocketBookeader Pro, enTourage eDGe, and Bebook Neo.
* BBC Micro built Texas Instruments TMS5220 speech synthesis chip,
* Some models of Texas Instruments home computers produced in 1979 and 1981 (Texas Instruments TI-99/4 and TI-99/4A) can be used to recite phoneme synthesis from text or complete words and phrases (dictionaries from text) using very popular voice synthesizer peripherals. I was able to do it. TI used its own codecs to embed complete voice phrases mainly in applications such as video games.
*IBM's OS/2Warp4 included VoiceType, a predecessor to IBM ViaVoice.
* GPS navigation units produced by Garmin, Magellan, TomTom and others use speech synthesis for automotive navigation.
* Yamaha produced the Yamaha FS1R music synthesizer with formant synthesis function in 1999. Up to 512 individual vowel and consonant formants sequences can be stored and played, allowing you to synthesize short voice phrases.
Digital Sound - alikes
At the 2018 Neural Information Processing Systems Conference (NeurIPS), Google researchers will present the "Multi-speaker Text-speech Synthesis from Speaker Verification", which transfers learning from speaker verification to achieve text-speech synthesis from speaker verification, which can make it sound like almost anyone from a voice sample of just 5 seconds. It is the first time that we have been able to do so.
Also, Baidu Research researchers presented a voice cloning system with a similar purpose at the 2018NeurIPS conference, but the results are not quite convincing.
As Symantec researchers know, by 2019, Digital Soundalix has found its way into the hands of criminals, as we know of 3 cases in which Digital Soundalix technology is being used in crime.
This increases the stress of disinformation situations coupled with the fact that
*Human Image synthesis Since the early 2000s, it has been improved from a human simulation imaged by a camera simulation to the point that it can't tell a real human imaged by a real camera.
* 2D video forgery technology was presented in 2016, which allows near-real-time forgery of facial expressions in existing 2D video.
*At SIGGRAPH 2017, AN AUDIO・DRIVEN DIGITAL LOOKALIKE OF BARACK OBAMA'S UPPER BODY WAS PRESENTED BY RESEARCHERS AT THE UNIVERSITY OF WASHINGTON. As the source data for the animation after the training phase, it is driven only by an audio track, gets lip sync, and consists of 2D video with audio.
In 2020 year 3 month, web15.ai It was released to produce high-quality voices from an assortment of fictional characters from various media sources. The first characters include GLaDOS from the Portal, My Little Pony from the Twilight Sparkle and Fluttershy Show: Friendship is magic and the tenth doctor from Doctor Who.
Text-to-speech markup language
Many markup languages have been established for the representation of text as voice in an XML-compliant format. The most recent was the Speech Synthesis Markup Language (SSML), which became the W3C recommendation in 2004. Older speech-to-speech markup languages include the Java Speech Markup Language (JSML) and SABLE. Each of these was proposed as a standard, but none has been widely adopted.
The speech-to-speech markup language is distinct from the interactive markup language. For example, VoiceXML includes tags related to speech recognition, conversation management, and touch-tone dialing in addition to text-to-speech markup.
Application
Speech synthesis has long been an important assistive technology tool, and its application in this field is important and extensive. It allows environmental barriers to be removed for people with a wide range of disabilities. While the longest application has been in the use of screen readers for people with visual impairments, text-to-speech systems are now commonly employed to help people with dyslexia and other reading normally, usually through a dedicated voice-output communication aid they also frequently use. Work is now available to personalize the synthetic voice according to a person's personality and historical voice. The featured application of speech synthesis is a text-to-speech software based on working from the black box synthesizer built by Haskins Laboratories and Votrax.
Speech synthesis technology is also used in entertainment works such as games and animation. In 2007, Animo Limited announced the development of a software application package based on the speech synthesis software FineSpeech. The application matured in 2008, and NEC Biglobe launched a web service that allows you to create phrases from the voices of characters from the Japanese anime series "Code Geass Rebel Lelouch R2".
In recent years, it has become widely available to convert text to speech for communication aids with disabilities and disabilities. For example, a combination of speech synthesis and speech recognition enables interaction with mobile devices through a natural language processing interface.
Text-to-speech is also used to learn a second language. For example, Voki is an educational tool created by Oddcast that allows users to create their own talking avatar using different accents. They can be sent via email, embedded on a website or shared on social media.
Another area of the application is AI video creation using talking heads. Synthesia allows users to create video content that includes AI avatars that are made to speak using text-to-speech technology.
In addition, speech synthesis is a valuable computational aid for the analysis and evaluation of speech disorders. Jorge C.A voice quality synthesizer developed by Lucero et al. The University of Brasilia simulates the physics of vocalization and includes models of voice frequency jitter and tremor, airflow noise, and laryngeal asymmetry. Synthesizers are used to mimic the tone of an abnormal speaker with controlled levels of roughness, breathiness and distortion.
In the 2010s, singing synthesis technology utilized recent advances in artificial intelligence — deep listening and machine learning - to better express the nuances of the human voice. The new high-fidelity sample library, combined with digital audio workstations, makes it easy to edit details, such as shifting formats, adjusting vibrato, and adjusting vowels and consonants. A sample library for various languages and various accents is available. With today's advances in song synthesis, artists sometimes use sample libraries instead of backing Singers.