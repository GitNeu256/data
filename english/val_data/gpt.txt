Generative pre-trained transformers (GPTs) are a type of large-scale language model (LLM) and a prominent framework for generative artificial intelligence. The first GPT was introduced by OpenAI in 2018.GPT models are artificial neural networks based on a transformer architecture, pre-trained on large datasets of unlabeled text and capable of generating novel human-like content As of 2023, most LLMs have these characteristics and are sometimes broadly referred to as GPTs.
OpenAI has released a series of sequentially numbered and highly influential GPT infrastructure models as the "GPT-n" series. Each of these models is significantly more capable than the previous one due to increased size (number of trainable parameters) and training. The latest GPT-4 was released in March 2023. These models are the basis for more task-specific GPT systems, including one that has been fine-tuned for instructional follow-up.
The term "GPT" is also used in the names and descriptions of such models developed by others. Examples include a series of GPT foundation models developed by EleutherAI and, more recently, seven GPT foundation models developed by Celebrus. In addition, companies from different industries have developed GPTs specialized for tasks in their respective fields, such as Salesforce's EinsteinGPT (for CRM) and Bloomberg's BloombergGPT (for finance).
History
Early Development
Generative pre-training (GP) has long been an established concept in machine learning applications, but the transformer architecture was not available until 2017, when it was invented by a Google employee. This development led to the emergence of large-scale language models like BERT in 2018, which were pre-trained transformers (PTs) but were not designed to be generative (BERT was an "encoder-only" model). Also around that time, in 2018, OpenAI published a paper titled "Improving Language Understanding by Generative Pre-Training," in which the first generative pre-trained transformer ("GPT") system ("GPT-1").
Prior to the advent of transformer-based architectures, the best performing neural NLP (natural language processing) models were typically learned by supervised learning from large amounts of manually labeled data. This reliance on supervised learning limited their use on insufficiently annotated data sets and made training very large language models prohibitively expensive and time-consuming.
The semi-supervised approach adopted by OpenAI to build large-scale generative systems, which was the first for transformational models, involved two stages: an unsupervised generative "pre-training" stage in which initial parameters are set using language modeling goals, and a target task in which these parameters are and a supervised identification "fine-tuning" phase to adapt these parameters to the target task.
For the more recent GPT foundation models, OpenAI released the first version of GPT-3 in July 2020, with three models with parameters of 1B, 6.7B, and 175B, named babbage, curie, and davinci (acronyms B, C, and D, respectively) The first model was
In July 2021, OpenAI announced Codex, a task-specific GPT model for programming applications. It was developed by tweaking a 12B parameter version of GPT-3 (different from the previous GPT-3 model) using code from GitHub.
In March 2022, OpenAI released two fine-tuned (instruction-tuned) versions of GPT-3 for instruction following, named davinci-instruct-beta (175B) and text-davinci-001. text-davinci-003 and ChatGPT were both released in November 2022, and both build on text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained to follow instructions (like its predecessor) while ChatGPT is further trained for conversational interaction with human users.
OpenAI's latest GPT base model, GPT-4, was released on March 14, 2023. In addition to direct user access through the premium version of ChatGPT, developers can incorporate it into other products and services through OpenAI's API Other producers of GPT infrastructure models include EleutherAI (launching a series of models in March 2021) and Cerebras ( (releasing seven models in March 2023).
Foundation Models
Foundational models are AI models that have been trained on large sets of data to be adaptable to a wide range of downstream tasks.
So far, the most notable GPT foundation models have been OpenAI's GPT-n series. The latest is GPT-4, for which OpenAI refuses to disclose scale or training details (citing "the competitive environment and the safety implications of large models").
Google's PaLM, an extensive underlying model that has been compared to GPT-3 and recently made available to developers through an API, and Together's GPT-JT, which has been reported as the closest performing open-source alternative to GPT-3 (from which the previous open-source GPT derived), etc. Meta AI (formerly Facebook) also has an underlying large language model based on generative transformation known as LLaMA.
Basic GPTs can also use modalities other than text for input and output; GPT-4 is a multimodal LLM that can process text and image input (but output is limited to text). For multimodal output, several generative transformation-based models have been used for text-to-image techniques such as diffusion and parallel decoding. These types of models can serve as visual foundational models (VFM) for developing downstream systems that can handle images.
Task Specific Models
The underlying GPT model can be further adapted to generate more targeted systems for specific task and/or subject matter domains. Such adaptation methods include additional fine-tuning (beyond what was done in the foundation model) and some form of prompt engineering.
An important example of this is fine-tuning the model to follow instructions. Of course, this is a much broader task, but more targeted than the foundation model; in January 2022, OpenAI announced "InstructGPT." InstructGPT" is a set of models based on the GPT-3 language model, fine-tuned to follow instructions through a combination of supervised learning and reinforcement learning from human feedback (RLHF). This had advantages such as higher accuracy, less negative/toxic sentiment, and generally better alignment with user needs. Hence, OpenAI began using it as the basis for its API service offerings. Other instruction-tuned models have been released by other companies, including fully open versions.
Another (related) kind of task-specific model is the human-like conversational chatbot: in November 2022, OpenAI released an online chat interface with instruction-tuned language models trained in a similar way to InstructGPT, called ChatGPT. They trained this model using RLHF to provide a conversation in which a human AI trainer acts as both user and AI, and mixed this new dialogue data set with the InstructGPT data set to create a conversational format suitable for chatbots. Currently, other major chatbots include Microsoft's Bing Chat, which uses OpenAI's GPT-4 (part of a close collaboration between OpenAI and Microsoft), and Google's competing chatbot Bard (initially a conversational learned language model from the LaMDA family based on the LaMDA family of conversational learned language models, but plans to switch to PaLM).
Yet another kind of task for which GPT can be used is a meta-task that generates its own instructions. For example, one could develop a set of prompts so that "self" can accomplish more general goals given by the human user. This is known as an AI agent, specifically a recursive agent.
Multi-modality
Generative transformation-based systems can also target tasks involving modalities other than text.
For example, Microsoft's Visual ChatGPT combines ChatGPT with a visual foundation model (VFM) to enable input and output consisting of images as well as text. In addition, advances in text-to-speech technology, when used in conjunction with the GPT language model, provide a powerful tool for spoken content creation.
Domain Specificity
GPT systems can be directed to specific fields or domains. Examples of such models and applications reported include:
* EinsteinGPT - for the sales and marketing sector, supporting customer relationship management (using GPT-3.5).
* BloombergGPT - for the financial sector, assisting with financial news and information (using a combination of "freely available" AI methods and the company's own data).
* Khanmigo - described as a GPT version for tutoring. In the education domain, it assists students using Khan Academy and provides tutoring without providing direct answers (using GPT-4).
* SlackGPT - for the instant messaging service Slack, to assist with navigation and summarization of discussions on Slack (using OpenAI's API).
* BioGPT - for the biomedical domain. Assists in text generation and mining of biomedical literature (using GPT-2).
Domain specificity may be achieved through software plug-ins and add-ons. For example, several different companies have developed specific plug-ins that interact directly with OpenAI's ChatGPT interface, and there are add-ons available for Google Workspace, such as "GPT for Sheets and Docs".
Brand Issues.
OpenAI, which created its first generative pre-trained transformer (GPT) in 2018, has recently argued that "GPT" should be considered OpenAI's brand; in April 2023, OpenAI revised the branding guidelines in its Terms of Service to allow its artificial intelligence (AI) services to run In May 2023, OpenAI contracted with a brand management service to notify its API customers of this policy, but these notifications were not subject to overt legal claims (e.g., trademark infringement allegations, or requests to cease and desist).
In this connection, OpenAI applied to the U.S. Patent and Trademark Office (USPTO) for a national trademark registration for the term "GPT" in the field of AI; OpenAI requested expedited processing of the application, but the USPTO denied that request in April 2023. In order to receive a trademark registration, OpenAI must prove that the term is actually "characteristic" of their particular product, rather than being widely understood as a broad technical term referring to this type of technology. Some reports suggest that OpenAI may be able to do so indirectly based on the reputation of their GPT-based chatbot product, ChatGPT. Other reports suggest that the term "GPT" is used so frequently simply to refer to AI systems that include generatively pre-trained converters that it seems unlikely that exclusivity would be granted. If exclusivity were granted for the term in the U.S., companies in the U.S. that use the term in the name or branding of related products would need to ensure that they do not use the term without permission. Even if this were to happen, the trademark doctrine of descriptive fair use would still leave room for continued non-brand-related use.