Large-scale language models (LLMs) are language models characterized by their size. Their size is achieved by AI accelerators, which can process huge amounts of text data scraped primarily from the Internet.LLMs are artificial neural networks that can contain from a billion to a trillion weights and are trained (in advance) using self-supervised or semi-supervised learning. trained). The transformer architecture contributed to faster training. An alternative architecture is the mixture of experts (MoE) proposed by Google in 2017.
As a language model, it works by taking input text and iteratively predicting the next token or word; until 2020, the only way to adapt the model to a specific task was fine-tuning. However, larger models such as GPT-3 can be promptly engineered to achieve similar results; GPT-3 will not only acquire the embodied knowledge of syntax, semantics, and "ontology" inherent in a human language corpus, but also the imprecision and bias present in the corpus. The three are believed to acquire not only the embodied knowledge of syntax, semantics, and "ontology" inherent in human language corpora, but also the inaccuracies and biases present in the corpus.
Notable examples include OpenAI's GPT models (such as GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM (used in Bard), Meta's LLaMa, BLOOM, Ernie 3.0 Titan, and Anthropic's Claude 2.
Preprocessing of datasets
Probabilistic tokenization
Using a modified byte-pair encoding, the first step treats all unique characters (including whitespace and punctuation) as an initial set of n-grams (i.e., an initial set of 1-grams). The most frequent pairs of adjacent characters are sequentially merged into a bi-gram, and all instances of that pair are replaced by the bi-gram. All occurrences of adjacent pairs of n-grams that occur most frequently together (previously merged) are repeatedly merged into even longer n-grams until a vocabulary of a given size is obtained (for GPT-3, the size is 50257). The token vocabulary consists of integers, ranging from 0 to the size of the token vocabulary. A new word can always be interpreted as a combination of a token and an initial monogram.
A token vocabulary based on frequencies extracted primarily from English corpora uses as few tokens as possible for the average English word. However, average words in other languages encoded by such an English-optimized tokenizer are split into the optimal amount of tokens.
Probabilistic tokenization also compresses the data set. This is why byte-pair encoding algorithms are used as tokenizers; LLM generally requires the input to be a non-jagged array, so short text must be "padded" until it matches the length of longer text. The average number of tokens required per word depends on the language of the dataset.
Cleaning the Dataset
Removing harmful sentences from a dataset, discarding low-quality data, and removing duplicates are examples of dataset cleaning. As a result, the cleaned (high-quality) dataset will contain up to 17 trillion words by 2022, up from 985 million words used in GPT-1 and 3.3 billion words used in BERT in 2018. However, future data will be increasingly "contaminated" by the LLM-generated content itself.
Learning and Architecture Details
Reinforcement Learning from Human Feedback (RLHF) with algorithms such as Proximity Policy Optimization will be used to further fine-tune the model based on a data set of human preferences.
Instructional Tuning.
LLM can bootstrap the correct response using a "self-instruction" approach, starting with modifications to a few human-generated cases, replacing any naive responses. For example, in the instruction "Write an essay on the main themes expressed in Hamlet," based on the frequency of this text sequence in the corpus, the first naive completion might be "Submit your essay after March 17 and your grade will drop by 10% for each day late." The first naive completion might be. Prompt Engineering, Attention Mechanisms, and Context Windows
Most results previously achievable only through (costly) fine-tuning can be achieved through prompt engineering, albeit limited to the scope of a single conversation (more precisely, the scope of a context window).
To find out which tokens are related to each other within the scope of a context window, the attention mechanism can be used to calculate soft weights for each token, or more precisely its embedding, by using multiple attention heads, each with its own "relevance" to calculate the For the embedding, the "soft" weights are computed. For example, the small version of the GPT-2 model (117M parameters) has 12 attention heads and a context window of only 1k tokens. The medium version of GPT-2 has 345M parameters and contains 24 layers, each layer having 12 attention heads. A batch size of 512 was used for training by gradient descent.
GPT-3.5 has a context window of 4k to 16k, while legacy GPT-3 has a context window of 2k.
The length of the conversation that the model can consider when generating the next answer is also limited by the size of the context window. For example, in Chat-GPT, if the length of the conversation is longer than the context window, only the inner part of the context window is considered when generating the next answer.
The disadvantages of increasing the context window size are higher computational cost and potentially less focus on the local context. Balancing these is a matter of experimentation and domain-specific considerations.
Models are pre-trained to predict, given a segment of the training data set, how that segment is followed or what is missing in that segment. The model can be one of the following.
* For example, given the segment "I like to eat," the model will predict "ice cream."
* For example, given the segment "I like to [__] [__] cream," the model predicts that "eat" and "ice" are missing.
Models may be trained on auxiliary tasks that test understanding of the data distribution, such as next sentence prediction (NSP), in which sentence pairs are presented and the model must predict whether they will appear consecutively in the training corpus. During training, regularization loss is also used to stabilize learning. However, regularization loss is not typically used during testing or evaluation.
Learning Cost
Advances in software and hardware have significantly reduced costs since 2020, so that in 2023, the learning cost for a 12-billion-parameter LLM is 72,300 A100-GPU hours, while in 2020, the learning cost for a 1.5-billion-parameter LLM (two orders of magnitude less than the 2020 technology level) is Learning costs ranged from $80,000 to $1.6 million; after 2020, huge sums were invested in increasingly larger models. For example, training the 2019 GPT-2 (i.e., 1.5 billion parameter model) cost $50,000 and training the 2022 PaLM (i.e., 54 billion parameter model) cost $8 million.
For the Transformer-based LLM, the training cost is much higher than the inference cost: it takes 6 FLOPs per parameter to train one token, whereas it takes 1-2 FLOPs per parameter to infer one token.
Use of Tools
In any LLM, there are tasks that cannot be solved in principle without at least some external tools or additional software. An example of such a task is the response to the input '354 * 139 = ' from the user. In such a case, the LLM would need to execute the program code that computes the result. Another example is the response to the question 'What time is it? In this case, another program interpreter needs to execute code to get the computer's system time, so the LLM can include it in the response. This basic strategy can be refined by multiple trials of the generated program or by using other sampling strategies.
In general, in order to get LLMs to use a tool, it must be fine-tuned to use the tool. If the number of tools is finite, fine-tuning may only need to be done once. If the number of tools is arbitrarily large, as in the case of online API services, then the LLM can read the API documentation and fine-tune it to call the API correctly.
A simpler way to use the tools is search extension generation: extend LLM with document search, possibly using a vector database. Given a query, a document search is invoked to find the most relevant ones (usually measured by first encoding the query and documents into a vector, then finding the document with the vector closest in Euclidean norm to the query vector). The LLM then generates output based on both the query and the retrieved documents.
Agency.
Although LLMs are language models and not agents because they do not have goals, they can be used as components of intelligent agents. Researchers have described several methods for such integration.
The ReAct ("Reason+Act") method uses the LLM as a planner to build an agent from the LLM; the LLM is prompted to "think aloud." Specifically, the language model is populated with a textual description of the environment, a goal, a list of possible actions, and a record of previous actions and observations. Before generating actions, one or more thoughts are generated, which are then executed in the environment; the linguistic description of the environment given to the LLM planner may even be LaTeX code for a paper describing the environment.
In the DEPS ("Describe, Explain, Plan, Select") method, the LLM is first connected to the visual world by an image description and then prompted to generate complex task and action plans based on pre-trained knowledge and received environmental feedback.
The Reflexion method builds an agent that learns over multiple episodes. At the end of each episode, LLMs are given a record of that episode and are prompted to consider "lessons learned" to perform better in the next episode. These "lessons learned" are then given to the agent in the next episode.
Monte Carlo tree search allows the LLM to be used as a rollout heuristic. If a programmed world model is not available, the LLM can also prompt a description of the environment to act as a world model.
For open-ended exploration, LLMs can be used to score the "interestingness" of observations, which can then be used as a reward signal to guide regular (non-LM) reinforcement learning agents. Alternatively, LLMs can suggest increasingly difficult tasks for curriculum learning. Instead of outputting individual behaviors, the LLM planner can also construct "skills," or functions of complex sequences of actions. Skills can be stored and recalled later, thus increasing the abstraction level of planning.
Agents equipped with LLMs can retain long-term memories of previous contexts, which can be retrieved in the same way as retrieval-extended generations. Multiple agents can interact socially.
Compression
Typically, LLMs are trained with full- or half-precision floating-point numbers (float32 and float16). One float16 is 16 bits, or 2 bytes, so loading 1 billion parameters requires 2 gigabytes. The largest models typically have 100 billion parameters and require 200 gigabytes to load.
Post-training quantization is intended to reduce the accuracy of the parameters of a learned model, thereby reducing the amount of space required while maintaining most of its performance. The simplest form of quantization is simply to truncate all numbers to a given number of bits. This can be improved by using different quantization codebooks for each layer. It can be further improved by applying different accuracies to different parameters and applying higher accuracies to particularly important parameters ("outlier weights").
Quantized models are usually frozen, and only pre-quantized models are fine-tuned, although quantized models can also be fine-tuned.
Multimodality
Multimodality means "with multiple modalities," and "modality" refers to the type of input: video, image, audio, text, proprioception, etc. There are many AI models that have been specifically trained to input one modality and output another, such as AlexNet from image to label, visual question answering from image to text, and speech recognition from audio to text. A review of multimodal LLMs follows.
A common way to create multimodal models from LLMs is to "tokenize" the output of trained encoders. Specifically, an LLM that understands images can be constructed as follows: take a learned LLM, take a learned image encoder E, create a small multilayer perceptron f, and for any image y, the processed vector f(E(y)) has the same dimension as the encoded token with the same dimensionality as the encoded tokens. This is the "image token. Text tokens and image tokens can then be interleaved. This composite model is then fine-tuned with the image-text data set. This basic structure can be applied to a higher degree to improve the model. The image encoder can also be frozen to improve stability.
Flamingo demonstrated the effectiveness of the tokenization method, tweaking a pre-trained language model and image encoder pair performed better on visual question answering than a model trained from scratch Google's PaLM model uses the tokenization method to create a multimodal model The LLaMA model was fine-tuned into PaLM-E and applied to robot control; the LLaMA model was also multimodalized using tokenization methods to enable image and video input.
GPT-4 is capable of both text and image input, and Google Gemini is also expected to be multimodal.