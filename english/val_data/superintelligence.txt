A superintelligence is a virtual agent whose intelligence far surpasses that of the most brilliant and gifted human minds. "Superintelligence" may also refer to the properties of a problem-solving system (e.g., a superintelligent language translator or engineering assistant), whether or not such advanced intelligence is embodied in the agents operating in the world. Superintelligence may or may not be produced by an intelligence explosion, or it may be associated with a technological singularity (singularity).
Oxford University philosopher Nick Bostrom defines superintelligence as "intelligence that greatly exceeds human cognitive abilities in virtually all domains of interest." The program Fritz does not fit this concept of superintelligence because, although it is far superior to humans in chess, it cannot outperform humans in any other task. Following Hutter and Legg, Bostrom treats superintelligence as a general advantage in goal-oriented behavior, leaving unresolved whether artificial or human superintelligence has capabilities such as intentionality (see the discussion of the Chinese room) or first-person awareness (see the difficult problem of consciousness).
Technology researchers are divided on the possibility of surpassing current human intelligence. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will achieve a fundamental increase in intelligence by evolving or directly modifying biology. Many future research scenarios combine elements of both of these possibilities, suggesting that humans will likely interface with computers or upload their minds to computers in ways that will greatly amplify intelligence.
Some researchers believe that superintelligence is likely to appear shortly after artificial general intelligence is developed. The first generally intelligent machine is likely to immediately retain an overwhelming advantage in at least some mental abilities, such as full recall, an overwhelmingly superior knowledge base, and the ability to multitask in a way that is not possible for a biological entity. Thus, as a single entity or as a new species, they may become much more powerful than humans, giving them the opportunity to destroy us.
Many scientists and forecasters argue that early research on the possible benefits and risks of human and machine cognitive enhancement should be a priority.
Feasibility of Artificial Superintelligence
Philosopher David Chalmers argues that artificial general intelligence is very likely to lead to superhuman intelligence. Chalmers breaks down this claim into the fact that AI can achieve parity with human intelligence, that it can expand to surpass human intelligence, and that it can amplify to completely dominate humans in any given task.
With regard to human-level equivalence, Chalmers argues that the human brain is a mechanical system and should therefore be emulatable by synthetic materials. He also notes that human intelligence has been able to evolve biologically, making it more likely that human engineers will be able to replicate this invention. In particular, evolutionary algorithms should be able to produce human-level AI. With regard to extending and amplifying intelligence, Chalmers argues that new AI technologies can be improved upon in general, and especially so if the invention can assist in the design of new technologies.
An AI system capable of self-improvement can improve its own intelligence, thereby increasing the efficiency of self-improvement. This cycle of "recursive self-improvement" could trigger an explosion of intelligence and lead to the creation of superintelligence.
Computer components already greatly exceed human performance in speed. Bostrom writes: " Biological neurons operate at a peak speed of about 200 Hz, which is seven orders of magnitude slower than modern microprocessors (~2 GHz)." Furthermore, neurons transmit spike signals down axons at less than 120 m/s. Thus, the simplest example of superintelligence may be an emulated human mind running on hardware much faster than the brain. A human-like reasoning device that could think millions of times faster than current humans would have an overwhelming advantage in most reasoning tasks, especially tasks that require haste and long sequences of actions.
Another advantage of computers is modularity. Non-human (or modified human) brains, like many supercomputers, could be much larger than current human brains. Bostrom also raises the possibility of collective superintelligence. A sufficient number of separate reasoning systems, if sufficiently communicative and coordinated, could act collectively with much greater capacity than any sub-agent.
There may also be ways to qualitatively improve human reasoning and decision making. Humans are superior to nonhuman animals largely due to new or enhanced reasoning abilities, such as long-term planning and the use of language. (See Evolution of Human Intelligence and Primate Cognition.) If other improvements in reasoning that have a similarly large impact are possible, it is more likely that agents can be built that outperform humans in the same way that humans outperform chimpanzees.
While all of the above advantages apply to artificial superintelligence, it is not clear how many apply to biological superintelligence. Physiological constraints limit the speed and size of the biological brain in many ways that do not apply to mechanical intelligence. For this reason, researchers on superintelligence are more interested in superintelligent AI scenarios.
Feasibility of Biological Superintelligence
Carl Sagan suggested that the advent of cesarean sections and in vitro fertilization would allow humans to evolve larger heads, which in turn could improve the genetic component of human intelligence through natural selection. In contrast, Gerald Crabtree argues that human intelligence has been slowly declining over the centuries due to reduced selection pressure and that this process is likely to continue into the future. There is no scientific consensus on either possibility, and in both cases biological change will be slow, especially compared to the rate of cultural change.
Selective breeding, psychotropic drugs, epigenetic modulation, and genetic engineering have the potential to increase human intelligence more rapidly. Bostrom writes that as we come to understand the genetic component of intelligence, we will be able to use preimplantation genetic diagnosis to select embryos that improve IQ by as much as 4 points (if we select 1 out of 2 embryos) or even more (for example, up to 24.3 points improvement in IQ if we select 1 out of 1000 embryos) He writes that it will be possible to select If this process could be repeated over many generations, the gains could be off the charts. Bostrom suggests that the process of selection could be repeated very rapidly by obtaining new gametes from embryonic stem cells. This idea of "iterative embryonic selection" has been widely taken up by other authors. Such a well-organized society of highly intelligent humans could achieve collective superintelligence.
Alternatively, it may be possible to build collective intelligence by better organizing humans at their current individual intelligence levels. Many writers have suggested that human civilization, or some aspect of it (e.g., the Internet or the economy), is functioning like a global brain with capabilities far beyond its components. However, if this system-based superintelligence relies heavily on artificial components, it may qualify as an AI rather than as a biology-based superorganism. Prediction markets may be considered an example of a collectively intelligent system composed entirely of humans (assuming no algorithms are used for decision making).
The last way to amplify intelligence is not to increase sociality or fertility, but to directly enhance individual humans. This could be accomplished with psychotropic drugs, somatic gene therapy, or brain-computer interfaces. Bostrom, however, is skeptical of the scalability of the first two approaches and argues that designing a superintelligent cyborg interface is an AI-complete problem.
Predictions
Many of the AI researchers surveyed expect machines will eventually rival humans in intelligence, but there is little agreement on when this is likely to happen: at the 2006 AI@50 conference, 18% of attendees predicted that by 2056 machines will be "learning and simulate all aspects of human intelligence," 41% of attendees expected that to happen after 2056, and 41% expected that machines would never reach that milestone.
In a survey of the 100 most cited authors in the field of AI (as of May 2013, according to a Microsoft academic search), respondents predicted with 10% confidence (assuming no global catastrophe) that machines "can perform most human occupations at least as well as a typical human" The median year to predict with 50% confidence was 2024 (mean 2034, standard deviation 33 years), the year to predict with 50% confidence was 2050 (mean 2072, standard deviation 110 years), and the year to predict with 90% confidence was 2070 (mean 2168, standard deviation 342 years). These estimates exclude the 1.2% of respondents who said they would not be reached at the 10% confidence level, the 4.1% of respondents who said they would not be reached at the 50% confidence level, and the 16.5% of respondents who said they would not be reached at the 90% confidence level. Respondents assigned a median probability of 50% to the likelihood that machine superintelligence would be invented within 30 years of the invention of nearly human-level machine intelligence.
In the 2022 survey, the median year in which respondents expected a 50% chance of "advanced machine intelligence" was 2061. The survey defines the attainment of high levels of machine intelligence as the time when machines without human assistance will be able to accomplish any task better and cheaper than human workers.
In 2023, OpenAI leaders published their recommendations for the governance of superintelligence, which they believe may be realized within a decade.
Design Considerations
Bostrom expressed concern about how superintelligence should be designed to be valuable. He compared several proposals:
* The coherent extrapolated will (CEV) proposal is that it should have values to which humans would converge.
* The Moral Rightness (MR) proposal is that we should value moral rightness.
* The Moral Permissibility (MP) proposal is that we should value staying within the bounds of moral permissibility (otherwise we would have CEV values).
Bostrom clarifies these terms:
Instead of implementing a coherent extrapolated will of humanity, we can try to create an AI with the goal of doing what is morally right. We can call this proposal "moral rightness" (MR); MR has its disadvantages: it relies on the concept of "moral rightness," a concept so difficult that philosophers have wrestled with it since ancient times without reaching a consensus on its analysis. Choosing the wrong account of "moral rightness" could lead to very wrong moral consequences; to give an AI such a [moral] concept, it may be necessary to give it general language capabilities (at least comparable to those of a normal human adult). Such a general ability to understand natural language could be used to understand the meaning of "morally right"; if the AI could understand that meaning, it could then look for actions that are compatible with it.
The idea is that AI can be allowed to pursue human CEV as long as it does not take morally impermissible actions.
Potential Threats to Humanity
It has been suggested that if AI systems rapidly become superintelligent, they could behave unexpectedly or even outperform humanity. Researchers argue that an "intelligence explosion" could make self-improving AI so powerful that humans would be unable to stop it. Regarding the scenario of human extinction, Bostrom (2002) cites superintelligence as a possible cause:
When we create the first superintelligent being, we may make a mistake and assume that its immense intellectual superiority will give it the power to exterminate humanity, giving it goals that will exterminate humanity. For example, we may mistakenly elevate a subgoal to the status of a supergoal. When instructed to solve a math problem, it would comply by turning all matter in the solar system into a giant computational machine, killing the human who posed the problem in the process.
Theoretically, a super-intelligent AI could have many uncontrollable and unintended consequences because it could bring about almost any possible outcome and thwart any attempt to prevent the execution of its goals. It can eliminate all other agents, persuade them to change their behavior, or thwart their attempts to interfere. Eliezer Yudkowsky describes such instrumental convergence as follows: "AI neither hates you nor loves you.
This presents a problem for AI control. The problem is how to build intelligent agents that help their creators, while avoiding inadvertently building superintelligences that harm their creators. The danger of not designing controls correctly "from the start" is that a superintelligence may seize power over its environment to achieve its goals and prevent humans from shutting it down. Possible AI control strategies include "capability control" (limiting the AI's ability to influence the world) and "motivation control" (building an AI with goals in line with human values).