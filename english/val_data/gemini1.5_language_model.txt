Last week we rolled out our most powerful model, Gemini 1.0 Ultra, a major step forward in making Gemini Advanced and other Google products more useful. Today, developers and cloud customers can start building with 1.0 Ultra using the Gemini API in AI Studio and Vertex AI.
Our team continues to push the frontiers of the latest models with safety at their core. They are making rapid progress. In fact, we are ready to introduce the next generation: Gemini 1.5. 1.5Pro shows dramatic improvements in many dimensions, achieving quality comparable to 1.0Ultra while using less compute.
This new generation also represents a breakthrough in understanding long context. We have been able to significantly increase the amount of information our models can process. We have consistently run up to 1 million tokens and achieved the longest context window of any large underlying model.
Longer context windows indicate the potential of what is possible. They enable entirely new capabilities and help developers build more useful models and applications. We are pleased to offer developers and enterprise customers a limited preview of this experimental functionality. Demis said of the features, security, and availability.
- Sundar
Introducing Gemini 1.5
These are exciting times for AI. New advances in the field have the potential to make AI useful to billions of people in the coming years, and since launching Gemini 1.0, we have tested, refined, and enhanced its capabilities.
Today we are announcing the next generation: Gemini 1.5.
Gemini 1.5 offers dramatically enhanced performance, is a step change in our approach, and is based on research and engineering innovation across nearly every part of our foundation model development and infrastructure. Gemini 1.5 is a step change in our approach, based on research and engineering innovations in almost every part of our Foundation Model development and infrastructure. This includes training and delivering Gemini 1.5 more efficiently through our new Mixture-of-Experts (MoE) architecture.
The first Gemini 1.5 model is Gemini 1.5 Pro, a medium-sized multimodal model optimized for scaling a variety of tasks and performing on par with 1.0 Ultra, the largest model to date. It also introduces groundbreaking experimental features in long-context understanding.
Gemini 1.5 Pro comes standard with a 128,000 token context window. However, starting today, a limited number of developers and enterprise customers can try Gemini 1.5 Pro with up to 1 million token context windows in a private preview through AI Studio and Vertex AI.
As we deploy the full 1 million token context window, we are actively working on optimizations to improve latency, reduce computational requirements, and improve the user experience. We look forward to you trying out this groundbreaking feature.
These ongoing advancements in our next-generation model will open up new possibilities for people, developers, and companies that use AI to create, discover, and build.
Highly Efficient Architecture
Gemini 1.5 builds on our cutting-edge work on Transformer and MoE architecture. While the traditional Transformer functions as one large neural network, the MoE model is divided into smaller "expert" neural networks.
Depending on the type of input given, the MoE model learns to selectively activate only the most relevant expert pathways in its neural network. This specialization greatly increases the efficiency of the model. Google has been an early adopter and pioneer of MoE technology for deep learning through research such as Sparsely-Gated MoE, GShard-Transformer, Switch-Transformer, and M4.
Our latest innovations in model architecture allow Gemini 1.5 to learn complex tasks more quickly and deliver training and services more efficiently while maintaining quality. These efficiencies allow our team to iterate, train, and deliver more advanced versions of Gemini faster than ever before, and we are committed to further optimization. Greater context, more useful capabilities
The AI model's "context window" consists of tokens, which are the building blocks for processing information. A token can be a word, image, video, audio, or code, in whole or in part. The larger the model's context window, the more information can be captured and processed at a given prompt.
A series of machine learning innovations have increased the capacity of the 1.5 Pro's context window far beyond the original 32,000 tokens of Gemini 1.0. It can now process up to 1 million tokens in production.
In other words, 1.5 Pro can process enormous amounts of information at once: an hour of video, 11 hours of audio, over 30,000 lines of codebase, and over 700,000 words. Our research has also successfully tested up to 10 million tokens.
Complex Reasoning for Huge Amounts of Information
1.5 Pro can seamlessly analyze, classify, and summarize large amounts of content within a given prompt. For example, given a 402-page transcript from the Apollo 11 mission to the moon, it can reason about the conversations, events, and details found throughout the document.
Gemini 1.5 Pro can understand, infer, and identify curious details about the 402-page transcript of the Apollo 11 mission to the moon.
Better Understanding and Reasoning Beyond Modalities
1.5 Pro can perform highly sophisticated comprehension and inference tasks for a variety of modalities, including video. For example, given a 44-minute silent film by Buster Keaton, the model can accurately analyze various plot points and events, and can reason about small details of the film that could easily be missed.
Gemini 1.5 Pro can identify scenes from a 44-minute silent Buster Keaton movie when given a simple line drawing as a reference to a real object.
Relevant Problem Solving with Longer Code Blocks
1.5 Pro can perform more relevant problem-solving tasks with longer blocks of code: given a prompt consisting of more than 100,000 lines of code, he or she can better reason through examples, suggest useful modifications, and provide explanations of how different parts of the code work given.
Gemini 1.5 Pro can reason through 100,000 lines of code and give useful solutions, fixes, and explanations.
Enhanced Performance
When tested on a comprehensive panel of text, code, image, audio, and video assessments, 1.5 Pro outperformed 1.0 Pro on 87% of the benchmarks used to develop large language models (LLMs). It also performed nearly as well as the 1.0 Ultra on the same benchmarks.
Gemini 1.5 Pro maintains a high level of performance even with larger context windows: in the Needle In A Haystack (NIAH) evaluation, when small text fragments containing specific facts or statements are deliberately placed within a longer text block, 1. 5 Pro found embedded text in as many as 1 million tokens of long data blocks 99% of the time.
Gemini 1.5 Pro also exhibits impressive "in-context learning" skills. This means that new skills can be learned from information given in long prompts without the need for additional fine-tuning. We tested this skill on the MTOB (Machine Translation from One Book) benchmark. Given a grammar manual in Karaman, a language with fewer than 200 speakers worldwide, the model learned to translate English into Karaman at the same level as someone learning from the same content.
Because 1.5 Pro's long context window is the first of its kind in a large model, we are continually developing new evaluations and benchmarks to test its novel capabilities.
For more information, see the Gemini 1.5 Pro Technical Report.
Extensive Ethics and Safety Testing
In line with our AI principles and strong safety policy, our models undergo extensive ethical and safety testing. We then integrate these research findings into our governance processes and model development and evaluation to continually improve our AI system.
Since launching Ultra 1.0 in December, our team has continued to refine the model and make it more secure for a broader release. We have also conducted novel research on safety risks and developed red team techniques to test the scope of potential harm.
Prior to the release of 1.5 Pro, we have taken the same responsible approach to deployment as we did with the Gemini 1.0 model, conducting extensive assessments that included content safety and representational hazards. In addition, we are developing further tests that take into account the novel long-context features of 1.5 Pro.
Building and Experimenting with the Gemini Model
We are committed to responsibly bringing each new generation of Gemini models to billions of people, developers, and companies worldwide.
Starting today, a limited preview of 1.5 Pro is available to developers and enterprise customers through AI Studio and Vertex AI. For more information, visit the Google for Developers blog and the Google Cloud blog.
The 1.5 Pro with the standard 128,000 token context window will be introduced as soon as this model is ready for widespread release. In the near future, we will introduce a pricing tier that starts with a standard 128,000 context window and scales up to 1 million tokens.
Early testers will be able to try the 1 million token context window for free during the test period, but we anticipate long wait times for this experimental feature. Significant improvements in speed are also being considered.
Developers interested in testing 1.5 Pro should sign up for AI Studio.