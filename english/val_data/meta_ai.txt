Meta AI is an artificial intelligence laboratory affiliated with Meta Platforms Inc. (formerly Facebook, Inc.) Meta AI intends to develop various forms of artificial intelligence and to improve augmented reality and artificial reality technologies. Meta AI is an academic research institute focused on generating knowledge for the AI community. This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications of its products.
History
MetaAI began as the Facebook Artificial Intelligence Research Institute (FAIR), based at the company's Menlo Park, California headquarters, in London, England, and in a new laboratory in Manhattan. FAIR was officially announced in September 2013. FAIR was founded by deep learning professor and Turing Prize winner, Jan Lukun of New York University, directed the project. In collaboration with NYU's Data Science Center, FAIR's initial goal was to study data science, machine learning, and artificial intelligence to "understand intelligence, discover its fundamental principles, and make machines significantly more intelligent. "FAIR's research has led to the development of face recognition, photo tagging, personalized pioneered technologies that lead to feed recommendations. Vladimir Vapnik, a pioneer in statistical learning, joined FAIR in 2014 and is a co-inventor of the Support Vector Machine and one of the developers of the Vapnik-Chervonenkis theory.
FAIR opened a laboratory in Paris, France in 2015, followed by smaller satellite labs in Seattle, Pittsburgh, Tel Aviv, Montreal, and London In 2016, FAIR partnered with Google, Amazon, IBM, and Microsoft to launch the Partnership on Artificial Intelligence to Benefit People and Society. The organization focuses on open licensing research, supporting ethical and efficient research practices, and discussing fairness, inclusiveness, and transparency.
In 2018, Jerome Pesenti, former CTO of IBM's Big Data Group, became FAIR's president, and Lukun stepped down to serve as chief AI scientist.In 2018, FAIR ranked the world's top organizations leading AI research, and was It ranked 25th in the Research Rankings 2019.FAIR surged to 8th in 2019 and maintained its 8th position in the 2020 rankings.FAIR had a staff of about 200 in 2018 and had set a goal to double that number by 2020.
FAIR's early work included research on learning model-enabled memory networks, self-supervised learning and generative adversarial networks, text classification and translation, and computer vision. in 2017, FAIR introduced the Torch deep learning module and the open source machine learning framework, PyTorch, which was released and subsequently used in several deep learning technologies, including Tesla's Autopilot and Uber's Pyro. Also in 2017, FAIR terminated a research project when an AI bot developed a language that was incomprehensible to humans, fueling conversations about dystopian fears of artificial intelligence running amok. However, FAIR clarified that it discontinued the study not out of fear, but because it had achieved its original goal of understanding how language is generated.
FAIR was renamed Meta AI following the rebranding from Facebook Inc. to Meta Platforms Inc.
In 2022, Meta AI predicted the 3D shapes of 600 million potential proteins in two weeks.
Current Research.
During a live event on February 23, 2022, Inside the Lab: the Meta AI team discussed major advances in artificial intelligence research and development. One such tool is BuilderBot, which allows users to generate virtual worlds using voice commands. Other tools include No Language Left Behind, a system that allows automatic translation between written languages, and Universal Speech Translator, a system that allows instant speech-to-speech translation. Computer Vision
MetaAI's computer vision research aims to extract information about the environment from digital images and videos. one example of computer vision technology developed by AI is panoptic segmentation, which not only recognizes objects in the foreground but also classifies background scenes. Meta AI aims to improve visual question-answering technology. In this technology, machines answer questions from human users about images.
Natural Language Processing and Conversational AI
Artificial intelligence communication requires machines to understand natural language and generate natural language. Meta-AI seeks to improve these techniques to improve safe communication, regardless of what language the user speaks. Therefore, the generalization of natural language processing (NLP) technology to other languages is a central issue. Thus, Meta AI is actively working on unsupervised machine translation; Meta AI aims to improve natural language interfaces by developing aspects of chat dialogue such as repetition, concreteness, response relevance, and question types, building personality into image captions, and generating creativity-based language The company is working on this project.
In 2018, Meta AI released PyText, an open source modeling framework specifically for NLP systems.
In 2023, Meta AI released and open-sourced LLaMA (Large Language Model Meta AI), a 65B-parameter large-scale language model.
Rankings and Recommendations
Facebook and Instagram use Meta AI research for ranking and recommendations in news feeds, ads, and search results; Meta AI has also released ReAgent, a toolset for generating decisions and evaluating user feedback.
Systems Research
Machine learning and AI rely on the development of new algorithms, software, and hardware technologies. Therefore, Meta AI's systems research team studies computer languages, compilers, and hardware applications.
Theoretical Research
Meta AI studies the mathematical and theoretical foundations of artificial intelligence. Meta AI has published papers on learning theory, optimization, and signal processing.
Hardware
MTIA v1 is Meta's first generation AI training and inference accelerator, developed specifically for Meta's recommendation workload; it is manufactured using TSMC's 7nm process technology and operates at 800MHz frequency. In terms of processing power, the accelerator offers 102.4 TOPS at INT8 precision and 51.2 TFLOPS at FP16 precision, with a thermal design power consumption (TDP) of 25W.
The accelerator is built around 64 processing elements (PEs) in an 8 x 8 arrangement, with on-chip and off-chip memory resources and necessary interconnects. Each PE contains two processor cores (one with vector extensions) and several fixed functional units optimized for tasks such as matrix multiplication, integration, data transfer, and nonlinear function computation. The processor cores utilize the RISC-V Open Instruction Set Architecture (ISA) and are extensively customized to perform the required computational and control tasks.
The accelerator's memory subsystem uses LPDDR5 for off-chip DRAM resources and is scalable up to 128GB. In addition, 128 MB of on-chip SRAM is shared by all PEs to accelerate access to frequently used data and instructions. The design encourages parallelism and data reuse, providing thread- and data-level parallelism (TLP and DLP), instruction-level parallelism (ILP), and memory-level parallelism (MLP).
MTIA accelerators are mounted on compact dual M.2 boards for easy integration into servers. The boards are connected to the host CPU via PCIe Gen4 x8 links and consume as low as 35W. Servers hosting these accelerators utilize the Open Compute Project's Yosemite V3 server specification. Each server is equipped with 12 accelerators, which are interconnected via a hierarchy of PCIe switches, allowing workloads to be distributed across multiple accelerators to run simultaneously.