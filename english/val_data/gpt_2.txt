Generative Pre-trained Transformer 2 (GPT-2) is a large-scale language model by OpenAI and the second in a series of foundational GPT models.GPT-2 is a dataset of over 7,000 self-published novels in a variety of genres It was pre-trained on BookCorpus and trained on a dataset of 8 million web pages; a partial release was made in February 2019, followed by the full release of the 1.5 billion parameter model on November 5, 2019.
GPT-2 was created as a "direct scale-up" of GPT-1, increasing both the number of parameters and the training dataset size by a factor of 10. GPT-2 is a general-purpose learner, and its ability to perform a variety of tasks was a result of its general ability to accurately predict the next item in a sequence. This allowed it to translate text, answer questions about a topic from text, summarize sentences from larger texts, and sometimes produce text output at a level indistinguishable from humans, but it could become repetitive and nonsensical when producing longer sentences. GPT-3 and GPT -4 models have been superseded and are no longer open source.
GPT-2, like its predecessor GPT-1 and successors GPT-3 and GPT-4, has a generative pre-trained transformer architecture and implements deep neural networks, specifically transformer models. The attention mechanism allows the model to selectively focus on segments of input text that are predicted to be most relevant. The model can significantly improve parallelization and outperforms previous benchmarks on RNN/CNN/LSTM-based models.
Training
The transformer architecture allows for massive parallelization, allowing GPT models to be trained on larger corpora than traditional NLP models The GPT-1 model demonstrated that this approach is feasible, but GPT-2 will require a very large corpus of further explore the emergent properties of the learned network. CommonCrawl, a large corpus generated by web crawling and previously used to train natural language processing systems, was considered due to its sheer size, but was rejected after further investigation revealed that it contained a large amount of semantic unknown content. Instead, OpenAI developed a new corpus known as WebText, which, rather than indiscriminately scraping content from the World Wide Web, by Reddit posts that had earned at least three upvotes before December 2017 It was generated by scraping only the linked pages. The corpus was then cleaned, HTML documents were parsed into plain text, duplicate pages were eliminated, and Wikipedia pages were removed (as their presence in many other data sets could induce overfitting).
The training cost for GPT-2 is known to be $256 per hour, but the time taken to complete the training is unknown. However, costs for comparable large-scale language models using transducer architectures have been documented in more detail; the training process for BERT and XLNet consumed $6,912 and $245,000 in resources, respectively.
Release.
GPT-2 was first announced on February 14, 2019; a February 2019 article in The Verge by James Vincent said that "the sentences it generates are usually easily identifiable as non-human," but is "one of the most exciting examples to date" of a language generation program and continued:
Give it a fake headline, and it will write the rest of the article with fake quotes and statistics. Give them the first line of a short story and they will tell you what happens to your character next. With the right prompts, you can even write fan fiction.
The Guardian described the output as "plausible newspaper prose," and Kelsey Piper of Vox said, "One of the coolest AI systems I've ever seen might be the one that puts me out of work. GPT-2's flexibility was described as "impressive" by The Verge, specifically noting its ability to translate text between languages, summarize lengthy articles, and answer trivia questions.
A University of Amsterdam study using the modified Turing Test found that, in at least some scenarios, participants were unable to distinguish between poems generated by GPT-2 and those written by humans.
Restrictions and Partial Disclosure
While previous OpenAI models were immediately available to the public, OpenAI initially refused to release the GPT-2 source code to the public when it released GPT-2 in February, citing the risk of malicious use. One commonly cited justification is that the text generated is usually entirely new and could be used by spammers to circumvent automated filters; OpenAI tweaked it to "generate endless positive (or negative) reviews of products" The GPT-2 version, which was tweaked to "generate an infinite number of positive (or negative) reviews of a product," was demonstrated.
It was also justified that GPT-2 could be used to generate obscene or racist text. Researchers like Jeremy Howard warned that the technology "completely fills Twitter, email, and the web with reasonably-sounding, contextually appropriate prose, drowning out and filtering out all other speech is impossible. "In response to GPT-2, the Allen Artificial Intelligence Institute presented a tool to detect "neural fake news."
Opinions were divided, however; a February 2019 article in The Verge argued that the threat posed by GPT-2 was exaggerated. Anima Anandkumar, a professor at Caltech and director of machine learning research at NVIDIA, said there is no evidence that GPT-2 has the ability to pose the threat that OpenAI describes, and that what they did was the "opposite of open" and that the release of the full model The Gradient published an open letter urging OpenAI to release the model, comparing the threat posed by text-generating AI to that posed by the printing press and calling Photoshop "a technology that, despite its potential for chaos, has (thankfully) He cited Photoshop as an example of "a technology that, despite its potential for chaos, has (thankfully) not disrupted modern society:
Thirty years later, society is relatively unscathed, despite Photoshop being so simple that even high school students can use it, and so ubiquitous that it can command verbs. Why? Because everyone knows Photoshop.
The release of the 774M
Although OpenAI did not release a fully trained model or trained corpus, the description of their methodology (and the free availability of the underlying technology) in prior publications made it possible for others to replicate GPT-2 as free software. One such replication, OpenGPT-2, was released in August 2019, along with a freely licensed version of WebText called OpenWebText; the cloud computing cost of OpenGPT-2 was estimated at approximately $50,000.
On August 20, 2019, OpenAI released a partial version of GPT-2 with 774 million parameters (about half the size of the full 1.5 billion parameter model).
The full version of the 1.5 billion released
Initial fears that GPT-2 would be widely abused did not materialize. The Verge wrote, "There are reasons to be skeptical of the claim that AI technology will lead to some sort of 'infopocalypse. For starters, there are already programs, or humans, that can generate large numbers of plausible sentences at a fraction of the cost." By November 2019, OpenAI had "so far seen no strong evidence of abuse," and a full version with 1.5 billion parameters was released on November 5, 2019.
Limitations
While GPT-2's ability to generate plausible natural language text is generally regarded positively, its shortcomings have also been noted, especially when generating sentences longer than a few paragraphs. The Verge similarly noted that GPT-2's longer samples tended to "deviate from the topic" and lacked overall coherence. The Register argued that "even a human reader would realize something was wrong after a while," and noted that "GPT-2 does not answer questions as well as other systems that rely on algorithms to extract and retrieve information.
The implementation of GPT-2 is resource-intensive. The full version of the model is over 5 gigabytes, making it difficult to embed locally in applications and consuming large amounts of RAM. In addition, running a single forecast can "occupy the CPU for several minutes at 100% utilization," and even GPU processing can "take several seconds per forecast. To mitigate these problems, Hugging Face created DistilGPT2, which uses distillation of knowledge to create smaller models that are "33% smaller and twice as fast" although "a few points lower on some quality benchmarks."
Applications and Subsequent Studies
GPT-2 had been used in a variety of applications and services, as well as entertainment, before the release of the full version. in June 2019, a subreddit called r/SubSimulatorGPT2 was created, and different subreddits trained on different GPT -2 instances made posts and replied to each other's comments, allowing us to observe a situation where "r/Bitcoin personified into AI was discussing in the machine learning derived spirit of r/ShittyFoodPorn". In July of the same year, a GPT-2-based software program was released that autocompletes lines of code in various programming languages and was described as a "game changer" by users.
In 2019, AI Dungeon, which uses GPT-2 to generate dynamic text adventures based on user input, was released; AI Dungeon now offers access to the GPT-3 maximum release API as an optional paid upgrade, The free version of the site uses the second maximum release of GPT-3.Latitude, the company founded around AI Dungeon, has raised $3.3 million in seed funding in 2021. Several websites offer interactive demonstrations of various instances of the GPT-2 and other transformer models.
In February 2021, a crisis center for troubled teens announced that it would use a GPT-2-derived chatbot to help train counselors to talk with simulated teens (this use is purely for internal purposes; GPT-2 itself does not communicate with teens (This use is purely for internal purposes; GPT-2 itself does not communicate with teens).
On May 9, 2023, OpenAI released a mapped version of GPT-2; OpenAI mapped each neuron of GPT-2 using its successor, GPT-4, to determine its function.
Performance and Evaluation
GPT-2 is now capable of performing a variety of tasks beyond simple text generation, thanks to the breadth of its data set and techniques. It was able to answer questions, summarize, and even perform translations between languages in various specific domains without being told how to do more than predict the next word in a sequence.
An example of generalized learning is GPT-2's ability to machine translate between French and English. For this task, GPT-2's performance was evaluated using the WMT-14 translation task; the GPT-2 training corpus contained very little French text. Non-English texts were intentionally removed when cleaning the dataset prior to training, resulting in only 10 MB of French that the model could train on out of the remaining 40,000 MB (mostly from foreign language citations in English posts and articles).
Despite this, GPT-2 achieved 5 BLEUs on the WMT-14 English-French test set (slightly below the score for translation by word-for-word substitution). It also outperformed some of the most recent (2017) unsupervised machine translation baselines on the French to English test set, with GPT-2 achieving 11.5 BLEUs. This remained below the best performance of the modern unsupervised approach (2019), which achieved 33.5 BLEUs. However, other models used large amounts of French text to achieve this result, and GPT-2 was estimated to have used a monolingual French corpus about 1/500th the size of comparable approaches.
	Number of architectures/parameters Training data
GPT-1 12 levels, 12-head Transformer decoder (no encoder), then linear-softmax. 0.12 billion BookCorpus: 4.5 GB text, from 7000 unpublished books of various genres.
GPT-2 GPT-1, but with modified normalization 1.5 billion WebText: 40 GB text, 8 million documents, from 45 million web pages upvoted on Reddit.
GPT-3 GPT-2 (but with modifications to allow greater scaling).	175,057 GB of plain text, 300 billion tokens of CommonCrawl, WebText, English Wikipedia, and two book corpora (Books1 and Books2).
GPT-2 was to be followed by GPT-3 with 175 billion parameters to be released in 2020 (source code not available); access to GPT-3 would be provided only through open AI and an API provided by Microsoft. GPT-4 would then follow.